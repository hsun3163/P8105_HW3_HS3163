---
title: "P8105_HW3_HS3163"
author: "Hao Sun"
date: "10/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)


library(tidyverse)
library(p8105.datasets)
library(httr)
library(jsonlite)
```

## Problem 1

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets using:

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):

```{r Problem 1 pt1 EDA }
####Load the data
data("instacart")
###Preping material
DOW_Name = c( "Saturday","Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

###Tidy up the table
instacart_tidy<-instacart%>%
  ###Factorized the aisle, department, and change order_dow into human readable factor
  mutate(
    aisle = forcats::fct_reorder(factor(aisle),aisle_id),
    department = forcats::fct_reorder(factor(department),department_id),
    product_name = forcats::fct_reorder(factor(product_name),product_id),
    ##According to the github hosting the data, 0 = Saturday
    order_dow = order_dow+1,
    order_dow = forcats::fct_reorder(factor(DOW_Name[order_dow]),order_dow)
  )
##Examing which variables are used to uniquely identify each entry.
instacart%>%
  select(order_id, add_to_cart_order)%>%distinct()%>%count()
instacart%>%
  select(order_id, product_id)%>%distinct()%>%count()
##Check whether order_id and user_id are in 1 to 1 relationship
instacart%>%
  select(order_id,user_id)%>%distinct()%>%count()
instacart%>%
  select(user_id)%>%distinct()%>%count()
##Check whether any users have ordered any product more than once within the dataset.
instacart%>%
  select(product_id,user_id)%>%distinct()%>%count()
instacart_tidy

```
This data set has 1,384,617 entries with 15 variables.Each entry enssentially records the addition of one product to the basket of one order, and was uniquely identified by the combination of order_id and add_to_cart_order or product_id.  Among the variables,  "order_id", and "add_to_cart_order" are used to identify each records. Considering the potential incidents of adding one product twice to the cart, granted not recorded in this dataset,"product_id" shall not be treated as a part of the index variables. Instead, "product_id" alone with "aisle_id" and "department_id" are metadata describing the content of each addition of one product. "product_name", "aisle" and "department" respectifully lable the aforementioned metadata for human readablity. Besides, "reordered","order_number","order_dow","days_since_prior_order" describe each orders' temperal attributes, from which comsumer behavior estimation may be derived. "eval_set" variables described the portion of the dataset among the original instacart data collections, and are not relevent to our analysis. 
 
It is worth mentioned that in the dataset, no user_id are associated with more than one order_id. After examning the full instacart collection, it was revealed that only one order from each user was added to the train eval_set. This data set also did not included the quantity of each product purchased. For instance, for the order with order_id 1, it was shown that the user,whose id is 112108m, added 8 products, ranging from "Bulgarian Yogurt" to "Organic Whole String Cheese", but there is not information indicating, say, the number of bottles of Bulgarian Yogurt the user bought.


*  How many aisles are there, and which aisles are the most items ordered from?
__Answer__ 
There are `r instacart%>%distinct(aisle)%>%count` aisles in the data set.

```{r pb1 p1}
#Finding which aisles are the most items ordered from
instacart_tidy%>%
##Count the number of items added to order from each aisles.
  count(aisle)%>%
##Finding the aisle that is correlated to the most items added to order from each aisles.
  filter(n == max(n))%>%
  pull(aisle)%>%as.character()
```
The fresh vegetable is the aisle the most items ordered from

It occurs that out of the 134 aisles, fresh vegetable is the aisle with the most item ordered from. This result is consistent with the logo, a carrot, and one of its advertising images on Apple AppStore, a bag of fresh vegetables, used by instacart APP.

* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items
ordered. Arrange aisles sensibly, and organize your plot so others can read it.

__Answer__
```{r pb 1 p2}
instacart_tidy%>%
##Count the number of items added to order from each aisles, label each aisle alone with the department it belonged to.
  group_by(department,aisle)%>%
  summarise(n())%>%
  group_by(department)%>%
  ##Rename the n() column
  mutate(items_ordered_from = `n()`)%>%
  select(-`n()`)%>%
##Finding the aisle with more than 10000 purchases.
  filter(items_ordered_from >= 10000)%>%
  ##Order the aisle by the number of its sales, also labed each aisle for their correspond department 
  ggplot(aes(x = reorder(aisle,items_ordered_from),
             y = items_ordered_from , 
             fill = department))+
  geom_col()+
  labs(
    title = "Number of item added to orders per aisle",
    x = "Aisle",
    y = "# of item added to orders"
  )+
##Flip the graph so that the name of each aisle become visible
  coord_flip()

```

The first three most popular aisles, and by large margins, are fresh vegetables, fresh fruits, and package vegetable fruits. All of the three aisles are from the produce department, which generally implies that "the products are fresh and in the same state as where and when they were harvested." It turns out that the users included in this dataset primarily rely on instacart to the acquired fresh products. It makes senes to order fresh products only when they are about to be consumed. Otherwise, it defeats the purpose of ordering fresh food. Therefore users are more likely to order items from the produce department in a different order, and, due to the lack of a "quantities per order" variable, inflate the popularity.




* Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r pb1 p3}
instacart_tidy%>%
  filter(aisle == "baking ingredients" | aisle == "dog food care"  | aisle == "packaged vegetables fruits" )%>%
##Count the number of items added to order from each aisles.
  count(aisle,product_name)%>%
  group_by(aisle)%>%
  arrange(desc(n),.by_group = TRUE)%>%
##Finding the top 3 most sold item from each aisle
filter(n>=
  ##Finding the largest among 
    max(
    ##Among  n
      n[
    ##that is neither the second largest
      n!=max(n[n!=max(n)]) & 
    ##Nor the first largest
      n!=max(n)]
      ))%>%
  knitr::kable()
```
It is apperant that the three aisles have quite a different popularity for there are a order of manitude of differences between the item  with the same rank popularity from each aisles.



* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r pb1 p4 }
instacart_tidy%>%
##Select anything that fell in the catagory of Pink Lady Apples or Coffee Ice Cream
  filter(str_detect(product_name,'Pink Lady Apples') | str_detect(product_name,'Coffee Ice Cream') )%>%
##Aggregate similar products
  mutate(product_name = 
           replace(
             product_name,
             str_detect(product_name,'Pink Lady Apples'),
            'Pink Lady Apples'))%>%
    mutate(product_name = 
           replace(
             product_name,
             str_detect(product_name,'Coffee Ice Cream'),
            'Coffee Ice Cream'))%>%
##Count the number of items added to order from each aisles.
  group_by(product_name,order_dow)%>%
  summarise(`mean hour of the day` = mean(order_hour_of_day))%>%
  mutate(`mean hour of the day` = round(`mean hour of the day`) )%>%
  pivot_wider(names_from = order_dow, values_from = `mean hour of the day`)%>%
    knitr::kable()

```
Generally, the meantime of the order of coffee ice cream is later than that of pink lady apples, which implies that users are more likely to consume Coffee Ice Cream later the day. However, taking the mean of the hours of the day may not be a good indicator on the consumer behavior because it is likely that the ordering of a product may take place both in the morning and at night and produce a mean of noon, which is an accurate description of the scenario. In this case, modes would be a more appropriate indicator to account for this effect.

## Problem 2

his problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.

```{r PB 2 Data loading, cache = TRUE}
data(brfss_smart2010)
View(brfss_smart2010)
```


First, do some data cleaning:
format the data to use appropriate variable names;
```{r pb2 pt 1}
brfss_smart2010_tidy<-brfss_smart2010%>%
  mutate(
##Make it more clear
    state_abbr = Locationabbr ,
##Remove the state prefix, which existed in another column
    counties_and_cities = str_remove_all(Locationdesc, ".. - "),
##Specify the nature of the data
    geo_coordinate = GeoLocation ,
##Class is confusing
    theme_of_intersted = Class
##Drop duplicated column
    )%>%select(-GeoLocation,-Locationdesc,-Locationabbr )%>%
  janitor::clean_names()

```

* focus on the “Overall Health” topic
```{r pb2 pt 2}
brfss_smart2010_tidy_focus<-brfss_smart2010_tidy%>%
  filter(topic == "Overall Health" )
```

* include only responses from “Excellent” to “Poor”
```{r pb2 pt 3}
brfss_smart2010_tidy_focus%>%
#Check what response are there.
  select(response)%>%distinct()%>%pull
```
It occurs that only responses from “Excellent” to “Poor” are included.

* organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r pb2 pt 4}
response_order <- factor(c("Excellent","Very good","Good","Fair","Poor"))%>%
forcats::fct_reorder(c(5,4,3,2,1))

brfss_smart2010_tidy_focus<-brfss_smart2010_tidy_focus%>%
  mutate(response = factor(response,levels = levels(response_order)))
```

Using this dataset, do or answer the following (commenting on the results of each):

* In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r pb2 pt 5.1}
states_2002<-brfss_smart2010_tidy_focus%>%
###Only intersted in year 2002 and 2010
  group_by(year,state_abbr,counties_and_cities)%>%summarise()%>%
  count(year,state_abbr)%>%
  filter(n>= 7)%>%
  filter(year == 2002)%>%pull(state_abbr)

```
The CT, FL, MA, NC, NJ, and PA states were observed at 7 or more locations in 2002

```{r pb2 pt 5.2}
states_2010<-brfss_smart2010_tidy_focus%>%
###Only intersted in year 2002 and 2010
  group_by(year,state_abbr,counties_and_cities)%>%summarise()%>%
  count(year,state_abbr)%>%
  filter(n>= 7)%>%
  filter(year == 2010)%>%pull(state_abbr)
```
The CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA states were observed at 7 or more locations in 2010

Five out of six of the states, except CT, that were observed at 7 or more locations still keep their places in 2010. There are 9 new states that were observed at 7 or more locations in 2010. `r stop`


Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 
```{r pb2 pt 5.3}
Mean_state_per_year<-brfss_smart2010_tidy_focus%>%
    filter(response == "Excellent")%>%
  ##NA value will cause the mean function to return a NA
  drop_na(data_value)%>%
  group_by(year,state_abbr)%>%summarise(averages = mean(data_value))%>%
  transmute(
            state = state_abbr,
            averages_data_values = round(averages,digits = 2))
```


Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r}
Mean_state_per_year%>%
ggplot(aes(x = year, y = averages_data_values, color = state))+
geom_line()
```



Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss_smart2010_tidy_focus%>% 
  ##Only intersted in NY
  filter(state_abbr == "NY")%>%
  ##Only intersted in year 2006 and 2010
  filter(year == 2006| year == 2010 )%>%
  ## Data_value for each response for each county or city
  ggplot(aes(x = response,y = data_value,fill = counties_and_cities))+
  geom_col(position = "dodge",colour = "black")+
  facet_grid(.~year)

```

## Problem 3
Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices measure “activity counts” in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The data can be downloaded here. In this spreadsheet, variables activity.* are the activity counts for each minute of a 24-hour day starting at midnight.

* Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).
```{r}
##Load data
accel_data <-read_csv("./Data/accel_data.csv")
colnames(accel_data)
#Tidy the data
accel_data_tidy<-
  accel_data%>%
  pivot_longer(activity.1:activity.1440, names_to = "minutes" , names_prefix = "activity.")%>%
  mutate(days_of_week = factor(day,levels =  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday")),
         minute_of_day = as.numeric(minutes),
         activity = value,
         hour_of_day = floor(minute_of_day/60),
         weekend_or_weekday = (days_of_week =="Saturday" |days_of_week == "Sunday"  ),
         weekend_or_weekday = factor(weekend_or_weekday, labels = c("weekday","weekend")))%>%
        select(-day,-minutes,-value)


```



* Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
```{r}
summary_accel<-accel_data_tidy_reordered%>%
  group_by(week,weekend_or_weekday,days_of_week,day_id)%>%
  summarise(sum_activity_per_day = sum(activity))
##The day ID in the data set does not make sense at all, therefore, a day order will be added to try to elucidated the confusion
summary_accel<-summary_accel%>%ungroup()%>%mutate(day_order = seq(1,35))
summary_accel%>%
  ggplot(aes(x= day_order, y = sum_activity_per_day, fill =weekend_or_weekday ))+
  geom_col()
  
  
  
  
```



* Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.


```{r}
summary_accel_by_hour<-accel_data_tidy_reordered%>%
  group_by(week,weekend_or_weekday,days_of_week,hour_of_day,day_id)%>%
  summarise(sum_activity_per_hour = sum(activity))
summary_accel_by_hour
##Order the hour in sequence again.
summary_accel_by_hour<-summary_accel_by_hour%>%ungroup()%>%mutate(hr_order = seq(1,875))

summary_accel_by_hour%>%
  ggplot(aes(x= hr_order , y = sum_activity_per_hour , color = days_of_week ))+
  geom_line()


```




