P8105\_HW3\_HS3163
================
Hao Sun
10/8/2019

## Problem 1

This problem uses the Instacart data. DO NOT include this dataset in
your local data directory; instead, load the data from the
p8105.datasets using:

The goal is to do some exploration of this dataset. To that end, write a
short description of the dataset, noting the size and structure of the
data, describing some key variables, and giving illstrative examples of
observations. Then, do or answer the following (commenting on the
results of each):

``` r
####Load the data
data("instacart")
###Preping material
DOW_Name = c( "Saturday","Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

###Tidy up the table
instacart_tidy<-instacart%>%
  ###Factorized the aisle, department, and change order_dow into human readable factor
  mutate(
    aisle = forcats::fct_reorder(factor(aisle),aisle_id),
    department = forcats::fct_reorder(factor(department),department_id),
    product_name = forcats::fct_reorder(factor(product_name),product_id),
    ##According to the github hosting the data, 0 = Saturday
    order_dow = order_dow+1,
    order_dow = forcats::fct_reorder(factor(DOW_Name[order_dow]),order_dow)
  )
##Examing which variables are used to uniquely identify each entry.
instacart%>%
  select(order_id, add_to_cart_order)%>%distinct()%>%count()
```

    ## # A tibble: 1 x 1
    ##         n
    ##     <int>
    ## 1 1384617

``` r
instacart%>%
  select(order_id, product_id)%>%distinct()%>%count()
```

    ## # A tibble: 1 x 1
    ##         n
    ##     <int>
    ## 1 1384617

``` r
##Check whether order_id and user_id are in 1 to 1 relationship
instacart%>%
  select(order_id,user_id)%>%distinct()%>%count()
```

    ## # A tibble: 1 x 1
    ##        n
    ##    <int>
    ## 1 131209

``` r
instacart%>%
  select(user_id)%>%distinct()%>%count()
```

    ## # A tibble: 1 x 1
    ##        n
    ##    <int>
    ## 1 131209

``` r
##Check whether any users have ordered any product more than once within the dataset.
instacart%>%
  select(product_id,user_id)%>%distinct()%>%count()
```

    ## # A tibble: 1 x 1
    ##         n
    ##     <int>
    ## 1 1384617

``` r
instacart_tidy
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # … with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <fct>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <fct>, aisle_id <int>,
    ## #   department_id <int>, aisle <fct>, department <fct>

This data set has 1,384,617 entries with 15 variables.Each entry
enssentially records the addition of one product to the basket of one
order, and was uniquely identified by the combination of order\_id and
add\_to\_cart\_order or product\_id. Among the variables, “order\_id”,
and “add\_to\_cart\_order” are used to identify each records.
Considering the potential incidents of adding one product twice to the
cart, granted not recorded in this dataset,“product\_id” shall not be
treated as a part of the index variables. Instead, “product\_id” alone
with “aisle\_id” and “department\_id” are metadata describing the
content of each addition of one product. “product\_name”, “aisle” and
“department” respectifully lable the aforementioned metadata for human
readablity. Besides,
“reordered”,“order\_number”,“order\_dow”,“days\_since\_prior\_order”
describe each orders’ temperal attributes, from which comsumer behavior
estimation may be derived. “eval\_set” variables described the portion
of the dataset among the original instacart data collections, and are
not relevent to our analysis.

It is worth mentioned that in the dataset, no user\_id are associated
with more than one order\_id. After examning the full instacart
collection, it was revealed that only one order from each user was added
to the train eval\_set. This data set also did not included the quantity
of each product purchased. For instance, for the order with order\_id 1,
it was shown that the user,whose id is 112108m, added 8 products,
ranging from “Bulgarian Yogurt” to “Organic Whole String Cheese”, but
there is not information indicating, say, the number of bottles of
Bulgarian Yogurt the user bought.

  - How many aisles are there, and which aisles are the most items
    ordered from?

**Answer** There are 134 aisles in the data set.

``` r
#Finding which aisles are the most items ordered from
instacart_tidy%>%
##Count the number of items added to order from each aisles.
  count(aisle)%>%
##Finding the aisle that is correlated to the most items added to order from each aisles.
  filter(n == max(n))%>%
  pull(aisle)%>%as.character()
```

    ## [1] "fresh vegetables"

The fresh vegetable is the aisle the most items ordered from

It occurs that out of the 134 aisles, fresh vegetable is the aisle with
the most item ordered from. This result is consistent with the logo, a
carrot, and one of its advertising images on Apple AppStore, a bag of
fresh vegetables, used by instacart APP.

  - Make a plot that shows the number of items ordered in each aisle,
    limiting this to aisles with more than 10000 items ordered. Arrange
    aisles sensibly, and organize your plot so others can read it.

**Answer**

``` r
instacart_tidy%>%
##Count the number of items added to order from each aisles, label each aisle alone with the department it belonged to.
  group_by(department,aisle)%>%
  summarise(n())%>%
  group_by(department)%>%
  ##Rename the n() column
  mutate(items_ordered_from = `n()`)%>%
  select(-`n()`)%>%
##Finding the aisle with more than 10000 purchases.
  filter(items_ordered_from >= 10000)%>%
  ##Order the aisle by the number of its sales, also labed each aisle for their correspond department 
  ggplot(aes(x = reorder(aisle,items_ordered_from),
             y = items_ordered_from , 
             fill = department))+
  geom_col()+
  labs(
    title = "Number of item added to orders per aisle",
    x = "Aisle",
    y = "# of item added to orders"
  )+
##Flip the graph so that the name of each aisle become visible
  coord_flip()
```

<img src="P8105_HW3_HS3163_files/figure-gfm/pb 1 p2-1.png" width="90%" />

The first three most popular aisles, and by large margins, are fresh
vegetables, fresh fruits, and package vegetable fruits. All of the three
aisles are from the produce department, which generally implies that
“the products are fresh and in the same state as where and when they
were harvested.” It turns out that the users included in this dataset
primarily rely on instacart to the acquired fresh products. It makes
senes to order fresh products only when they are about to be consumed.
Otherwise, it defeats the purpose of ordering fresh food. Therefore
users are more likely to order items from the produce department in a
different order, and, due to the lack of a “quantities per order”
variable, inflate the popularity.

  - Make a table showing the three most popular items in each of the
    aisles “baking ingredients”, “dog food care”, and “packaged
    vegetables fruits”. Include the number of times each item is ordered
    in your table. **Answer**

<!-- end list -->

``` r
instacart_tidy%>%
  filter(aisle == "baking ingredients" | aisle == "dog food care"  | aisle == "packaged vegetables fruits" )%>%
##Count the number of items added to order from each aisles.
  count(aisle,product_name)%>%
  group_by(aisle)%>%
  arrange(desc(n),.by_group = TRUE)%>%
##Finding the top 3 most sold item from each aisle
filter(n>=
  ##Finding the largest among 
    max(
    ##Among  n
      n[
    ##that is neither the second largest
      n!=max(n[n!=max(n)]) & 
    ##Nor the first largest
      n!=max(n)]
      ))%>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n |
| :------------------------- | :-------------------------------------------- | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |
| baking ingredients         | Pure Baking Soda                              |  387 |
| baking ingredients         | Cane Sugar                                    |  336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |
| dog food care              | Small Dog Biscuits                            |   26 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |

It is apperant that the three aisles have quite a different popularity
for there are a order of manitude of differences between the item with
the same rank popularity from each aisles.

  - Make a table showing the mean hour of the day at which Pink Lady
    Apples and Coffee Ice Cream are ordered on each day of the week;
    format this table for human readers (i.e. produce a 2 x 7 table).

<!-- end list -->

``` r
instacart_tidy%>%
##Select anything that fell in the catagory of Pink Lady Apples or Coffee Ice Cream
  filter(str_detect(product_name,'Pink Lady Apples') | str_detect(product_name,'Coffee Ice Cream') )%>%
##Aggregate similar products
  mutate(product_name = 
           replace(
             product_name,
             str_detect(product_name,'Pink Lady Apples'),
            'Pink Lady Apples'))%>%
    mutate(product_name = 
           replace(
             product_name,
             str_detect(product_name,'Coffee Ice Cream'),
            'Coffee Ice Cream'))%>%
##Count the number of items added to order from each aisles.
  group_by(product_name,order_dow)%>%
  summarise(`mean hour of the day` = mean(order_hour_of_day))%>%
  mutate(`mean hour of the day` = round(`mean hour of the day`) )%>%
  pivot_wider(names_from = order_dow, values_from = `mean hour of the day`)%>%
    knitr::kable()
```

| product\_name    | Saturday | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday |
| :--------------- | -------: | -----: | -----: | ------: | --------: | -------: | -----: |
| Pink Lady Apples |       13 |     11 |     12 |      14 |        12 |       13 |     12 |
| Coffee Ice Cream |       13 |     14 |     15 |      15 |        15 |       12 |     14 |

Generally, the meantime of the order of coffee ice cream is later than
that of pink lady apples, which implies that users are more likely to
consume Coffee Ice Cream later the day. However, taking the mean of the
hours of the day may not be a good indicator on the consumer behavior
because it is likely that the ordering of a product may take place both
in the morning and at night and produce a mean of noon, which is an
accurate description of the scenario. In this case, modes would be a
more appropriate indicator to account for this effect.

## Problem 2

his problem uses the BRFSS data. DO NOT include this dataset in your
local data directory; instead, load the data from the p8105.datasets
package.

``` r
data(brfss_smart2010)
```

First, do some data cleaning: format the data to use appropriate
variable names;

``` r
brfss_smart2010_tidy<-brfss_smart2010%>%
  mutate(
##Make it more clear
    state_abbr = Locationabbr ,
##Remove the state prefix, which existed in another column
    counties_and_cities = str_remove_all(Locationdesc, ".. - "),
##Specify the nature of the data
    geo_coordinate = GeoLocation ,
##Class is confusing
    theme_of_intersted = Class
##Drop duplicated column
    )%>%select(-GeoLocation,-Locationdesc,-Locationabbr )%>%
  janitor::clean_names()
```

  - focus on the “Overall Health” topic

<!-- end list -->

``` r
brfss_smart2010_tidy_focus<-brfss_smart2010_tidy%>%
  filter(topic == "Overall Health" )
```

  - include only responses from “Excellent” to “Poor”

<!-- end list -->

``` r
brfss_smart2010_tidy_focus%>%
  ##Selecting only the desired responses.
  filter(response == "Excellent" |response == "Very good" |response == "Good" |response == "Fair"| response == "Poor")
```

    ## # A tibble: 10,625 x 24
    ##     year class topic question response sample_size data_value
    ##    <int> <chr> <chr> <chr>    <chr>          <int>      <dbl>
    ##  1  2010 Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 Heal… Over… How is … Very go…         148       30  
    ##  3  2010 Heal… Over… How is … Good             208       33.1
    ##  4  2010 Heal… Over… How is … Fair             107       12.5
    ##  5  2010 Heal… Over… How is … Poor              45        5.5
    ##  6  2010 Heal… Over… How is … Excelle…          91       15.6
    ##  7  2010 Heal… Over… How is … Very go…         177       31.3
    ##  8  2010 Heal… Over… How is … Good             224       31.2
    ##  9  2010 Heal… Over… How is … Fair             120       15.5
    ## 10  2010 Heal… Over… How is … Poor              66        6.4
    ## # … with 10,615 more rows, and 17 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, state_abbr <chr>,
    ## #   counties_and_cities <chr>, geo_coordinate <chr>,
    ## #   theme_of_intersted <chr>

  - organize responses as a factor taking levels ordered from “Poor” to
    “Excellent”

<!-- end list -->

``` r
response_order <- factor(c("Excellent","Very good","Good","Fair","Poor"))%>%
forcats::fct_reorder(c(5,4,3,2,1))
brfss_smart2010_tidy_focus<-brfss_smart2010_tidy_focus%>%
  mutate(response = factor(response,levels = levels(response_order)))
```

Using this dataset, do or answer the following (commenting on the
results of each):

  - In 2002, which states were observed at 7 or more locations? What
    about in 2010?

<!-- end list -->

``` r
states_2002<-brfss_smart2010_tidy_focus%>%
###Only intersted in year 2002 and 2010
  group_by(year,state_abbr,counties_and_cities)%>%summarise()%>%
  count(year,state_abbr)%>%
  filter(n>= 7)%>%
  filter(year == 2002)%>%pull(state_abbr)
```

The CT, FL, MA, NC, NJ, and PA states were observed at 7 or more
locations in 2002

``` r
states_2010<-brfss_smart2010_tidy_focus%>%
###Only intersted in year 2002 and 2010
  group_by(year,state_abbr,counties_and_cities)%>%summarise()%>%
  count(year,state_abbr)%>%
  filter(n>= 7)%>%
  filter(year == 2010)%>%pull(state_abbr)
```

The CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA states
were observed at 7 or more locations in 2010

Five out of six of the states, except CT, that were observed at 7 or
more locations still keep their places in 2010. There are 9 new states
that were observed at 7 or more locations in 2010.

  - Construct a dataset that is limited to Excellent responses, and
    contains, year, state, and a variable that averages the data\_value
    across locations within a state.

<!-- end list -->

``` r
Mean_state_per_year<-brfss_smart2010_tidy_focus%>%
    filter(response == "Excellent")%>%
  ##NA value will cause the mean function to return a NA
  drop_na(data_value)%>%
  group_by(year,state_abbr)%>%summarise(averages = mean(data_value))%>%
  transmute(
            state = state_abbr,
            averages_data_values = round(averages,digits = 2))
Mean_state_per_year
```

    ## # A tibble: 443 x 3
    ## # Groups:   year [9]
    ##     year state averages_data_values
    ##    <int> <chr>                <dbl>
    ##  1  2002 AK                    27.9
    ##  2  2002 AL                    18.5
    ##  3  2002 AR                    24.1
    ##  4  2002 AZ                    24.1
    ##  5  2002 CA                    22.7
    ##  6  2002 CO                    23.1
    ##  7  2002 CT                    29.1
    ##  8  2002 DC                    29.3
    ##  9  2002 DE                    20.9
    ## 10  2002 FL                    25.7
    ## # … with 433 more rows

Make a “spaghetti” plot of this average value over time within a state
(that is, make a plot showing a line for each state across years – the
geom\_line geometry and group aesthetic will help).

``` r
Mean_state_per_year%>%
ggplot(aes(x = year, y = averages_data_values, color = state))+
geom_point()+geom_line()
```

<img src="P8105_HW3_HS3163_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />
It occurs that the state WY generally has the lowest Excellent response
value among all the states, especially in the year 2005. IN was doing ok
before 2007, but it dropped quite drastically in 2007. Overall, DC has
the best performance throughout the years. UT and CT were started high
but fell quite sharply in the coming years. After that, CT stayed
average, but UT rose up again after 2005.

  - Make a two-panel plot showing, for the years 2006, and 2010,
    distribution of data\_value for responses (“Poor” to “Excellent”)
    among locations in NY State.

<!-- end list -->

``` r
brfss_smart2010_tidy_focus%>% 
  ##Only intersted in NY
  filter(state_abbr == "NY")%>%
  ##Only intersted in year 2006 and 2010
  filter(year == 2006| year == 2010 )%>%
  ## Data_value for each response for each county or city
  ggplot(aes(x = response,y = data_value,fill = counties_and_cities))+
  geom_col(position = "dodge",colour = "black")+
  facet_grid(.~year)
```

<img src="P8105_HW3_HS3163_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

## Problem 3

Accelerometers have become an appealing alternative to self-report
techniques for studying physical activity in observational studies and
clinical trials, largely because of their relative objectivity. During
observation periods, the devices measure “activity counts” in a short
period; one-minute intervals are common. Because accelerometers can be
worn comfortably and unobtrusively, they produce around-the-clock
observations.

This problem uses five weeks of accelerometer data collected on a 63
year-old male with BMI 25, who was admitted to the Advanced Cardiac Care
Center of Columbia University Medical Center and diagnosed with
congestive heart failure (CHF). The data can be downloaded here. In this
spreadsheet, variables activity.\* are the activity counts for each
minute of a 24-hour day starting at midnight.

  - Load, tidy, and otherwise wrangle the data. Your final dataset
    should include all originally observed variables and values; have
    useful variable names; include a weekday vs weekend variable; and
    encode data with reasonable variable classes. Describe the resulting
    dataset (e.g. what variables exist, how many observations, etc).

<!-- end list -->

``` r
##Load data
accel_data <-read_csv("./Data/accel_data.csv")
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
##Tidy the data
accel_data_tidy<-
  accel_data%>%
  pivot_longer(activity.1:activity.1440, names_to = "minutes" , names_prefix = "activity.")%>%
  mutate(days_of_week = factor(day,levels =  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday")),
         minute_of_day = as.numeric(minutes),
         activity = value,
         hour_of_day = floor(minute_of_day/60),
         weekend_or_weekday = (days_of_week =="Saturday" |days_of_week == "Sunday"  ),
         weekend_or_weekday = factor(weekend_or_weekday, labels = c("weekday","weekend")))%>%
        select(-day,-minutes,-value)
```

  - Traditional analyses of accelerometer data focus on the total
    activity over the day. Using your tidied dataset, aggregate accross
    minutes to create a total activity variable for each day, and create
    a table showing these totals. Are any trends apparent?

<!-- end list -->

``` r
summary_accel<-accel_data_tidy%>%
  ##Finding the sum of activity of each day
  group_by(week,weekend_or_weekday,days_of_week,day_id)%>%
  summarise(sum_activity_per_day = sum(activity))
##The day ID in the data set does not make sense at all, therefore, a day order will be added to try to elucidated the confusion
summary_accel<-summary_accel%>%ungroup()%>%mutate(day_order = seq(1,35))

##Ploting the graph
summary_accel%>%
  ggplot(aes(x= day_order, y = sum_activity_per_day, fill =weekend_or_weekday ))+
  geom_col()+
labs(
    title = "Total activity per day",
    x = "Time Progression(Day)",
    y = " Total Activity" )
```

<img src="P8105_HW3_HS3163_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

It occurs that the activity of the man is more or less consistent on the
weekdays by varied a lot during the weekend.

  - Accelerometer data allows the inspection activity over the course of
    the day. Make a single-panel plot that shows the 24-hour activity
    time courses for each day and use color to indicate day of the week.
    Describe in words any patterns or conclusions you can make based on
    this graph.

<!-- end list -->

``` r
summary_accel_by_hour<-accel_data_tidy%>%
  group_by(week,weekend_or_weekday,days_of_week,hour_of_day,day_id)%>%
  summarise(sum_activity_per_hour = sum(activity))
summary_accel_by_hour
```

    ## # A tibble: 875 x 6
    ## # Groups:   week, weekend_or_weekday, days_of_week, hour_of_day [875]
    ##     week weekend_or_weekd… days_of_week hour_of_day day_id sum_activity_pe…
    ##    <dbl> <fct>             <fct>              <dbl>  <dbl>            <dbl>
    ##  1     1 weekday           Monday                 0      2               59
    ##  2     1 weekday           Monday                 1      2               60
    ##  3     1 weekday           Monday                 2      2               60
    ##  4     1 weekday           Monday                 3      2               60
    ##  5     1 weekday           Monday                 4      2               60
    ##  6     1 weekday           Monday                 5      2               60
    ##  7     1 weekday           Monday                 6      2               60
    ##  8     1 weekday           Monday                 7      2               60
    ##  9     1 weekday           Monday                 8      2               60
    ## 10     1 weekday           Monday                 9      2               60
    ## # … with 865 more rows

``` r
##Order the hour in sequence again.
summary_accel_by_hour<-summary_accel_by_hour%>%ungroup()%>%mutate(hr_order = seq(1,875))%>%
    ##Find sum activity per week,which is used for potential normalization that control for weekly differences.
  group_by(week)%>%mutate(sum_activity_per_week = sum(sum_activity_per_hour))

summary_accel_by_hour%>%
  ggplot(aes(x= hr_order , y = sum_activity_per_hour , fill = days_of_week ))+
 geom_col()+
labs(
    title = "24-hour normalized activity time courses for each day",
    x = "Time Progression(Hr)", 
    y = "Total Activity" )
```

<img src="P8105_HW3_HS3163_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

Before any conclusion was made, it is worth pointing out the abnormality
of the activity data on Monday of the first week and Saturday of the
fourth and fifth weeks. Instead of complete inactivity. A more likely
explanation of such anomalies may be the malfunction or detach of
Accelerometer on Saturdays.

According to the produced graphs, generally speaking, the man usually
wakes up at 5 to 6 am, where a shape increase in activity can be
observed. The man usually sleeps after 10 pm, and only on rare occasions
did he sleep before 11. Throughout the weeks, the pattern man’s activity
on Wed, Fri, and Sun is relatively more consistent than the other days
of the week. It occurs that the man may have a habit of exercise during
Friday night and Sunday morning, indicated by the peak at 8 pm on
Friday, and 10 am Sunday, respectively. The abnormal activity on Friday
of the fourth week may be due to the malfunction of the accelerometer as
well. For the first three weeks, the man occasionally takes naps at
noon, indicated by the sudden drops of activity around 11 am to 1 pm.
